{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# imports from custom library\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "import matplotlib.pyplot as plt\n",
    "import autograd.numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "%matplotlib notebook\n",
    "\n",
    "# this is needed to compensate for %matplotlib notebook's tendancy to blow up images when plotted inline\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.autolayout'] = True\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Unsupervised Learning Series\n",
    "\n",
    "#  Part 1: Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Motivation:\n",
    "\n",
    "$\\boxed{1}$ In many modern applications of machine learning, data is high-dimensional, producing:\n",
    "- computational problems in the training of predictive models\n",
    "- harder to interpret models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "PCA is a classical technique for reducing the feature dimension of a given dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\boxed{2}$ PCA presents a fundamental mathematical archetype, the matrix factorization, that provides a useful way of organizing our thinking about a wide array of important learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Press the button 'Toggle code' below to toggle code on and off for entire this presentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>jQuery(function() {if (jQuery(\"body.notebook_app\").length == 0) { jQuery(\".input_area\").toggle(); jQuery(\".prompt\").toggle();}});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onclick=\"jQuery('.input_area').toggle(); jQuery('.prompt').toggle();\">Toggle code</button>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "from IPython.display import HTML\n",
    "import IPython.core.display as di # Example: di.display_html('<h3>%s:</h3>' % str, raw=True)\n",
    "\n",
    "# This line will hide code by default when the notebook is exported as HTML\n",
    "di.display_html('<script>jQuery(function() {if (jQuery(\"body.notebook_app\").length == 0) { jQuery(\".input_area\").toggle(); jQuery(\".prompt\").toggle();}});</script>', raw=True)\n",
    "\n",
    "# This line will add a button to toggle visibility of code blocks, for use with the HTML export version\n",
    "di.display_html('''<button onclick=\"jQuery('.input_area').toggle(); jQuery('.prompt').toggle();\">Toggle code</button>''', raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "PCA works by simply projecting the data onto a suitable lower dimensional feature subspace, that is one which hopefully preserves the essential geometry of the original data. This subspace is found by determining one of its spanning sets (e.g., a basis) of vectors which spans it.\n",
    "\n",
    "<figure>\n",
    "  <img src= '../../../mlrefined_images/unsupervised_images/Fig_9_5.png' width=\"30%\" height=\"auto\" alt=\"\"/>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1.1 Notation and modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Suppose that we have $P$ data points $\\mathbf{x}_{1}, \\mathbf{x}_{2}, \\ldots, \\mathbf{x}_{P}$, each of dimension $N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The goal with PCA is, for some user chosen dimension $K<N$, to find a set of $K$ vectors $\\mathbf{c}_{1}, \\mathbf{c}_{2}, \\dots, \\mathbf{c}_{K}$ that represent the data fairly well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Put formally, we want for each $p=1, 2, \\ldots, P$ to have\n",
    "\n",
    "\\begin{equation}\n",
    "\\underset{k=1}{\\overset{K}{\\sum}}\\mathbf{c}_{k}w_{k,p}\\approx\\mathbf{x}_{p}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Stacking the desired spanning vectors column-wise into the $N\\times K$ matrix $\\mathbf{C}$ as\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{C}=\\left[\\begin{array}{cccc}\n",
    "\\mathbf{c}_{1} & \\mathbf{c}_{2} & \\cdots & \\mathbf{c}_{K}\\end{array}\\right]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "and denoting\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{w}_{p}=\\left[\\begin{array}{c}\n",
    "w_{1,p}\\\\\n",
    "w_{2,p}\\\\\n",
    "\\vdots\\\\\n",
    "w_{K,p}\n",
    "\\end{array}\\right]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "we have, for each $p$, that\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{C}\\mathbf{w}_{p}\\approx\\mathbf{x}_{p}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "More *compactification!* Denote the $K\\times P$ weight matrix\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{W}=\\left[\\begin{array}{cccc}\n",
    "\\mathbf{w}_{1} & \\mathbf{w}_{2} & \\cdots & \\mathbf{w}_{P}\\end{array}\\right]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "and the $N\\times P$ data matrix\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{X}=\\left[\\begin{array}{cccc}\n",
    "\\mathbf{x}_{1} & \\mathbf{x}_{2} & \\cdots & \\mathbf{x}_{P}\\end{array}\\right]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "to write all $P$ approximate equalities of the form $\\mathbf{C}\\mathbf{w}_{p}\\approx\\mathbf{x}_{p}$ more compactly (in a single equation) as\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{C}\\mathbf{W}\\approx\\mathbf{X}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The goal of PCA: learn matrices $\\mathbf{C}$ and $\\mathbf{W}$ such that\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{C}\\mathbf{W}\\approx\\mathbf{X}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This naturally leads to determining $\\mathbf{C}$ and $\\mathbf{W}$ by minimizing the following\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\\underset{\\mathbf{C},\\mathbf{W}}{\\mbox{minimize}} & \\,\\,\\,\\,\\,\\left\\Vert \\mathbf{C}\\mathbf{W}-\\mathbf{X}\\right\\Vert _{F}^{2}\\end{aligned}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <span style=\"color:#a50e3e;\">Example 1: </span> PCA on simulated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<figure>\n",
    "  <img src= '../../../mlrefined_images/unsupervised_images/Fig_9_6.png' width=\"55%\" height=\"auto\" alt=\"\"/>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <span style=\"color:#a50e3e;\">Example 2: </span> PCA and classification data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "While PCA can technically be used for preprocessing data in a predictive modeling scenario, it can cause severe problems in the case of classification, by completely destroying the separation structure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<figure>\n",
    "  <img src= '../../../mlrefined_images/unsupervised_images/Fig_9_7.png' width=\"55%\" height=\"auto\" alt=\"\"/>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <span style=\"color:#a50e3e;\">Example 3: </span> Role of efficient bases in digital image compression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<figure>\n",
    "  <img src= '../../../mlrefined_images/unsupervised_images/Fig_9_8.png' width=\"50%\" height=\"auto\" alt=\"\"/>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In a typical image compression scheme an input image is first cut up into small square (typically $8 \\times 8$ pixel) blocks. The values of pixels in each block are stacked into a column vector $y$, and compression is then performed on these individual vectorized blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The basic idea: with specific bases like DCT (left panel - used in JPEG), we only need very few of their elements to very closely approximate any natural image. This is because each DCT basis element represents a fluctuation commonly seen across the entirety of a natural image block. The same cannot be said of other bases with more locally defined elements like the standard basis shown on the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<figure>\n",
    "  <img src= '../../../mlrefined_images/unsupervised_images/Fig_9_9.png' width=\"50%\" height=\"auto\" alt=\"\"/>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**What is the difference between PCA and JPEG image compression?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Even though we're still aiming to reduce the dimension of data, instead of seeking out a basis (as with PCA), here we have a fixed basis (DCT) over which image data can be very efficiently represented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In other words, denoting by $\\mathbf{x}_{k}$ the $k^{th}$ (vectorized) image block, we solve $K$ linear systems of the form\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{C}\\mathbf{w}_{k}=\\mathbf{x}_{k}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{C}$ is known (and no longer learned). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "After solving $\\mathbf{C}\\mathbf{w}_{k}=\\mathbf{x}_{k}$, most of the weights in the found coefficient vector $\\mathbf{w}_{k}$ are typically quite small - so we set them to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<figure>\n",
    "  <img src= '../../../mlrefined_images/unsupervised_images/Fig_9_10.png' width=\"90%\" height=\"auto\" alt=\"\"/>\n",
    "  <figcaption>   \n",
    "<strong>Figure:</strong> <em> From left to right, the original $256 \\times 256$ input image along with its three compressed versions where we keep only the largest $20\\%$, $5\\%$, and $1\\%$ of the DCT coefficients to represent the image, resulting in compression by a factor of $5$, $20$, and $100$, respectively.\n",
    " </em>  </figcaption> \n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1.2 Optimization of the PCA problem "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recall the PCA optimization problem: \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\\underset{\\mathbf{C},\\mathbf{W}}{\\mbox{minimize}} & \\,\\,\\,\\,\\,\\left\\Vert \\mathbf{C}\\mathbf{W}-\\mathbf{X}\\right\\Vert _{F}^{2}\\end{aligned}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- This problem is convex in each variable $\\mathbf{C}$ and $\\mathbf{W}$ individually but non-convex in both simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We solve it via **alternation minimization**: alternately keep one variable fix and solve for the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Beginning at an initial value for $\\mathbf{C}=\\mathbf{C}^{\\left(0\\right)}$, we find $\\mathbf{W}^{\\left(1\\right)}$ as\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{array}{c}\n",
    "\\begin{aligned}\\mathbf{W}^{\\left(1\\right)}=\\,\\, & \\underset{\\mathbf{W}}{\\mbox{argmin}}\\,\\,\\left\\Vert \\mathbf{C}^{\\left(0\\right)}\\mathbf{W}-\\mathbf{X}\\right\\Vert _{F}^{2}\\end{aligned}\n",
    "\\end{array}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Setting the gradient to zero, this has a closed form solution given by\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{W}^{\\left(1\\right)}=\\left(\\left(\\mathbf{C}^{\\left(0\\right)}\\right)^{T}\\mathbf{C}^{\\left(0\\right)}\\right)^{\\dagger}\\left(\\mathbf{C}^{\\left(0\\right)}\\right)^{T}\\mathbf{X}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\left(\\cdot\\right)^{\\dagger}$ denotes the pseudo-inverse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Keeping $\\mathbf{W}$ fixed at $\\mathbf{W}^{\\left(1\\right)}$, we now minimize the PCA cost over $\\mathbf{C}$, giving  \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\\,\\,\\,\\,\\,\\mathbf{C}^{\\left(1\\right)}=\\,\\, & \\underset{\\mathbf{C}}{\\mbox{argmin}}\\,\\,\\left\\Vert \\mathbf{C}\\mathbf{W}^{\\left(1\\right)}-\\mathbf{X}\\right\\Vert _{F}^{2}\\end{aligned}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "with the closed form solution given by\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{C}^{\\left(1\\right)}=\\mathbf{X}\\left(\\mathbf{W}^{\\left(1\\right)}\\right)^{T}\\left(\\mathbf{W}^{\\left(1\\right)}\\left(\\mathbf{W}^{\\left(1\\right)}\\right)^{T}\\right)^{\\dagger}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This procedure is repeated, and only stopped after taking a certain number of iterations and/or when the subsequent iterations do not change significantly.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Alternating minimization algorithm for PCA\n",
    "\n",
    "<hr style=\"height:1px;border:none;color:#555;background-color:#555;\">\n",
    "<p style=\"line-height: 1.7;\">\n",
    "<strong>1:</strong>&nbsp;&nbsp; <strong>Input:</strong> data matrix $\\mathbf{X}$, initial $\\mathbf{C}^{\\left(0\\right)}$, and maximum number of iterations $J$\n",
    "<br>\n",
    "\n",
    "<strong>2:</strong>&nbsp;&nbsp; <code>for</code> $\\,\\,i = 1,\\ldots,J$<br>\n",
    "\n",
    "<strong>3:</strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; update the weight matrix $\\mathbf{W}$ via $\\mathbf{W}^{\\left(i\\right)}=\\left(\\left(\\mathbf{C}^{\\left(i-1\\right)}\\right)^{T}\\mathbf{C}^{\\left(i-1\\right)}\\right)^{\\dagger}\\left(\\mathbf{C}^{\\left(i-1\\right)}\\right)^{T}\\mathbf{X}$<br>\n",
    "\n",
    "<strong>4:</strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; update the principal components matrix $\\mathbf{C}$ via $\\mathbf{C}^{\\left(i\\right)}=\\mathbf{X}\\left(\\mathbf{W}^{\\left(i\\right)}\\right)^{T}\\left(\\mathbf{W}^{\\left(i\\right)}\\left(\\mathbf{W}^{\\left(i\\right)}\\right)^{T}\\right)^{\\dagger}$<br>\n",
    "\n",
    "<strong>5:</strong>&nbsp;&nbsp; <code>end for</code><br>\n",
    "\n",
    "<strong>6:</strong>&nbsp;&nbsp; <strong>output:</strong> $\\mathbf{C}^{\\left(J\\right)}$ and $\\mathbf{W}^{\\left(J\\right)}$ <br>\n",
    "\n",
    "<hr style=\"height:1px;border:none;color:#555;background-color:#555;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The SVD solution\n",
    "\n",
    "Denoting the singular value decomposition of $\\mathbf{X}$ as $\\mathbf{X}=\\mathbf{U}\\mathbf{S}\\mathbf{V}^{T}$, a closed form solution to the PCA problem can be found as\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{array}{c}\n",
    "\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\mathbf{C}^{\\star}=\\mathbf{U}_{K}\\mathbf{S}_{K,K}\\\\\n",
    "\\mathbf{W}^{\\star}=\\mathbf{V}_{K}^{T}\n",
    "\\end{array}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{U}_{K}$ and $\\mathbf{V}_{K}$ denote the matrices formed by the first $K$ columns of the left and right singular matrices $\\mathbf{U}$ and $\\mathbf{V}$ respectively, and $\\mathbf{S}_{K,K}$ denotes the upper $K\\times K$ sub-matrix of the singular value matrix $\\mathbf{S}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## 1.3 PCA from the directional variance perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Here we show how to derive PCA from a different perspective, as the orthogonal directions of largest variance in data. While this perspective is not particularly useful when using PCA as a preprocessing technique (since all we care about is reducing the dimension of the feature space, and so any basis spanning a proper subspace will suffice), it is often used in exploratory data analysis in fields like statistics and the social sciences (see e.g., factor analysis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Adopting the same notation as in Subsection 1.1, we have a dataset of $P$ points $\\left\\{ \\mathbf{x}_{p}\\right\\} _{p=1}^{P}$ - each of dimension $N$. Given a unit direction $\\mathbf{d}$, we can calculate the variance of data in that direction (i.e., how much the dataset spreads out in that direction) as the average squared inner product of the data against $\\mathbf{d}$\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{1}{P}\\underset{p=1}{\\overset{P}{\\sum}} \\left(\\mathbf{d}^T\\mathbf{x}_{p}\\right) ^{2}\n",
    "\\end{equation}\n",
    "\n",
    "Putting data points as columns into a matrix $\\mathbf{X}$ as in equation (6), this can be written more compactly as\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{1}{P}\\Vert\\mathbf{X}^{T}\\mathbf{d}\\Vert_{2}^{2}=\\frac{1}{P}\\mathbf{d}^{T}\\mathbf{X}\\mathbf{X}^{T}\\mathbf{d}\n",
    "\\end{equation} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "With the expression in (15) at hand, we can find the unit direction $\\mathbf{d}$ which maximizes it. This direction will be, by definition, the direction of maximum variance in data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let us assume that the $N\\times N$ symmetric positive semi-definite matrix $\\mathbf{X}\\mathbf{X}^{T}$ has eigenvalues $\\lambda_{1}\\geq\\lambda_{2}\\geq\\cdot\\cdot\\cdot\\geq\\lambda_{N}\\geq0$ and corresponding eigenvectors $\\mathbf{a}_{1}, \\mathbf{a}_{2}, \\ldots, \\mathbf{a}_{N}$, with an eigen-decomposition\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{X}\\mathbf{X}^{T}=\\mathbf{A}\\boldsymbol{\\Lambda}\\mathbf{A}^{T}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{A}$ is an orthonormal basis whose columns are the eigenvectors, and $\\boldsymbol{\\Lambda}$ is a diagonal matrix with the corresponding eigenvalues along its diagonal. With this eigen-decomposition we can write\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{d}^{T}\\mathbf{X}\\mathbf{X}^{T}\\mathbf{d}=\\mathbf{d}^{T}\\mathbf{A}\\boldsymbol{\\Lambda}\\mathbf{A}^{T}\\mathbf{d}=\\mathbf{r}^{T}\\boldsymbol{\\Lambda}\\mathbf{r}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{r}=\\mathbf{A}^{T}\\mathbf{d}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Expressing $\\mathbf{r}^{T}\\boldsymbol{\\Lambda}\\mathbf{r}$ in terms of individual entries in $\\mathbf{r}$ and $\\boldsymbol{\\Lambda}$ we have \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{d}^{T}\\mathbf{X}\\mathbf{X}^{T}\\mathbf{d}={\\sum}_{n=1}^N\\lambda_{n}r_{n}^{2}\\leq{\\sum}_{n=1}^N\\lambda_{1}r_{n}^{2}=\\lambda_{1}\\Vert\\mathbf{r}\\Vert_{2}^{2}=\\lambda_{1}\\mathbf{d}^{T}\\mathbf{A}\\mathbf{A}^{T}\\mathbf{d}=\\lambda_{1}\\mathbf{d}^{T}\\mathbf{d}=\\lambda_{1}\n",
    "\\end{equation}\n",
    "\n",
    "where we use the fact that $\\mathbf{A}$ is an orthonormal basis and $\\mathbf{d}$ has unit length.\n",
    "\n",
    "Notice, if you set $\\mathbf{d}=\\mathbf{a}_{1}$ then the objective meets its upper bound in (18), since\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{a}_{1}^{T}\\mathbf{X}\\mathbf{X}^{T}\\mathbf{a}_{1}=\\mathbf{a}_{1}^{T}\\mathbf{A}\\boldsymbol{\\Lambda}\\mathbf{A}^{T}\\mathbf{a}_{1}=\\mathbf{e}_{1}^{T}\\mathbf{\\boldsymbol{\\Lambda}e}_{1}=\\lambda_{1}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{e}_{1}=\\left[\\begin{array}{ccccc}\n",
    "1 & 0 & 0 & \\cdots & 0\\end{array}\\right]^{T}$ is the first standard basis vector, and we indeed have that\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\\,\\,\\,\\,\\,\\mathbf{a}_{1}=\\,\\, & \\underset{\\Vert\\mathbf{d}\\Vert_{2}=1}{\\mbox{argmax}}\\,\\,\\mathbf{d}^{T}\\mathbf{X}\\mathbf{X}^{T}\\mathbf{d}\n",
    "\\end{aligned}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "A similar approach can be taken to find the second largest direction of variance of the matrix $\\mathbf{X}\\mathbf{X}^{T}$, i.e., the unit vector $\\mathbf{d}$ that maximizes the value of $\\mathbf{d}^{T}\\mathbf{X}\\mathbf{X}^{T}\\mathbf{d}$ but where $\\mathbf{d}$ is also orthogonal to $\\mathbf{a}_1$ - the first largest direction of variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We now write the inequality in (18), slightly differently, as\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{d}^{T}\\mathbf{X}\\mathbf{X}^{T}\\mathbf{d}=\\lambda_{1}r_{1}^{2}+{\\sum}_{n=2}^N\\lambda_{n}r_{n}^{2}\\leq\\lambda_{2}r_{1}^{2}+{\\sum}_{n=2}^{N}\\lambda_{2}r_{n}^{2}={\\sum}_{n=1}^{N}\\lambda_{2}r_{n}^{2}\n",
    "\\end{equation}\n",
    "\n",
    "where the inequality holds since we are only looking for directions perpendicular to $\\mathbf{a}_{1}$, therefore $r_{1}=\\mathbf{d}^{T}\\mathbf{a}_{1}=0$, and we also have that $\\lambda_{2}\\geq\\cdot\\cdot\\cdot\\geq\\lambda_{N}\\geq0$ by assumption.\n",
    "\n",
    "Hence, we have \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{d}^{T}\\mathbf{X}\\mathbf{X}^{T}\\mathbf{d}\\leq{\\sum}_{n=1}^{N}\\lambda_{2}r_{n}^{2}=\\lambda_{2}\\Vert\\mathbf{r}\\Vert_{2}^{2}=\\lambda_{2}\\mathbf{d}^{T}\\mathbf{A}\\mathbf{A}^{T}\\mathbf{d}=\\lambda_{2}\\mathbf{d}^{T}\\mathbf{d}=\\lambda_{2}\n",
    "\\end{equation}\n",
    "\n",
    "Notice, this time, the upper bound is met with $\\mathbf{d}=\\mathbf{a}_{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Similarly, we can show that the rest of the orthogonal directions of maximum variance in data are the remaining eigenvectors of $\\mathbf{X}\\mathbf{X}^{T}$, which are precisely the singular value solution given previously in equation (13). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To see why this is true, we expand $\\mathbf{XX}^{T}$ using its singular value decomposition $\\mathbf{X}=\\mathbf{U}\\boldsymbol{S}\\mathbf{V}^{T}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\\begin{equation}\n",
    "\\mathbf{XX}^{T}=\\mathbf{U}\\boldsymbol{S}\\mathbf{V}^{T}\\left(\\mathbf{U}\\boldsymbol{S}\\mathbf{V}^{T}\\right)^{T}=\\mathbf{U}\\boldsymbol{S}\\mathbf{V}^{T}\\left(\\mathbf{V}\\boldsymbol{S}^{T}\\mathbf{U}^{T}\\right)=\\mathbf{U}\\boldsymbol{S}\\left(\\mathbf{V}^{T}\\mathbf{V}\\right)\\boldsymbol{S}^{T}\\mathbf{U}^{T}=\\mathbf{U}\\boldsymbol{S}\\boldsymbol{S}^{T}\\mathbf{U}^{T}\n",
    "\\end{equation}\n",
    "\n",
    "Given the eigen-decomposition of $\\mathbf{X}\\mathbf{X}^{T}$ as $\\mathbf{X}\\mathbf{X}^{T}=\\mathbf{A}\\boldsymbol{\\Lambda}\\mathbf{A}^{T}$ in equation (16), a simple inspection reveals that $\\mathbf{U}=\\mathbf{A}$ and $\\mathbf{S}\\mathbf{S}^{T}=\\boldsymbol{\\Lambda}$."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "214px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 1,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
