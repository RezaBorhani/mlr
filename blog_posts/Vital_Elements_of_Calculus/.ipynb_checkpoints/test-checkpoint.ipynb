{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Vector and matrix norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector norms give us a way to define ``distance'' between two vectors\n",
    "(or points) living in a general $N$ dimensional space. Let $\\mathbf{x}$\n",
    "and $\\mathbf{y}$ be two such vectors. Then the vector norm of their\n",
    "difference $\\mathbf{e}=\\mathbf{x}-\\mathbf{y}$, denoted by $\\left\\Vert \\mathbf{e}\\right\\Vert $,\n",
    "is a non-negative scalar indicating how large the difference between\n",
    "$\\mathbf{x}$ and $\\mathbf{y}$ is. The larger the $\\left\\Vert \\mathbf{e}\\right\\Vert $\n",
    "the more distant $\\mathbf{x}$ and $\\mathbf{y}$ are. The notion of\n",
    "distance is extremely useful in machine learning (as well as other\n",
    "areas of science) as it enables us to measure similarity of two vectors\n",
    "are to each other in a general $N$ dimensional space. Here we discuss\n",
    "popular vector norms that will arise frequently in our study of machine\n",
    "learning and deep learning. \n",
    "\n",
    "Let $\\mathbf{x}$ and $\\mathbf{y}$ be two such vectors. Then the\n",
    "vector norm of their difference $\\mathbf{e}=\\mathbf{x}-\\mathbf{y}$,\n",
    "denoted by $\\left\\Vert \\mathbf{e}\\right\\Vert $, is a non-negative\n",
    "scalar indicating how large the difference between $\\mathbf{x}$ and\n",
    "$\\mathbf{y}$ is. The larger the $\\left\\Vert \\mathbf{e}\\right\\Vert $\n",
    "the more distant $\\mathbf{x}$ and $\\mathbf{y}$ are.\n",
    "\n",
    "\\textemdash \\textemdash \\textemdash \\textendash{}\n",
    "\n",
    "A vector norm is a kind of function that measures the length of real\n",
    "vectors. Here we discuss popular vector norms that will arise frequently\n",
    "in our study of machine learning and deep learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vector norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. The $\\ell_{2}$ norm \n",
    "\n",
    "We begin with the most widely-used vector norm in machine learning, the $\\ell_{2}$ norm,\n",
    "defined for an $N$ dimensional vector $\\mathbf{x}$ as\n",
    "\n",
    "\\begin{equation}\n",
    "\\left\\Vert \\mathbf{x}\\right\\Vert _{2}=\\sqrt{\\sideset{}{_{n=1}^{N}}\\sum x_{n}^{2}}\n",
    "\\end{equation}\n",
    "\n",
    "Using the $\\ell_{2}$ norm we can measure the distance between any\n",
    "two points $\\mathbf{x}$ and $\\mathbf{y}$ via $\\left\\Vert \\mathbf{x}-\\mathbf{y}\\right\\Vert _{2}$,\n",
    "which is simply the length of the vector connecting $\\mathbf{x}$\n",
    "and $\\mathbf{y}$.\n",
    "\n",
    "For example, the distance between $\\mathbf{x}=\\left[\\begin{array}{c}\n",
    "1\\\\\n",
    "2\n",
    "\\end{array}\\right]$ and $\\mathbf{y}=\\left[\\begin{array}{c}\n",
    "9\\\\\n",
    "8\n",
    "\\end{array}\\right]$ is calculated as $\\sqrt{\\left(1-9\\right)^{2}+\\left(2-8\\right)^{2}}=10$,\n",
    "as shown pictorially in the figure below (recall Pythagorean theorem). \n",
    "\n",
    "Perhaps suprisingly, this is not the only way to measure, or more\n",
    "precisely \\emph{define}, the length of a vector. There are other norms\n",
    "arising occasionally in machine learning that we discuss below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\\section{The $\\ell_{1}$ norm }\n",
    "\n",
    "The $\\ell_{1}$ norm of a vector $\\mathbf{x}$ is defined as the sum\n",
    "of the absolute values of its entries\n",
    "\n",
    "\\begin{equation}\n",
    "\\Vert\\mathbf{x}\\Vert_{1}=\\sideset{}{_{n=1}^{N}}\\sum\\left|x_{n}\\right|\n",
    "\\end{equation}\n",
    "\n",
    "In terms of the $\\ell_{1}$ norm the distance between $\\mathbf{x}$\n",
    "and $\\mathbf{y}$ is given by $\\left\\Vert \\mathbf{x}-\\mathbf{y}\\right\\Vert _{1}$,\n",
    "which provides a measurement of distance different from the $\\ell_{2}$\n",
    "norm. As illustrated in the figure below the distance defined by the\n",
    "$\\ell_{1}$ norm is the length of a path consisting of perpendicular\n",
    "pieces (shown in blue). Because these paths are somewhat akin to how\n",
    "an automobile might travel from $\\mathbf{x}$ to $\\mathbf{y}$ if\n",
    "they were two locations in a gridded city, having to traverse perpindicular\n",
    "city blocks one after the other, the $\\ell_{1}$ norm is sometimes\n",
    "referred to as the taxicab norm, and the distance measured via the\n",
    "$\\ell_{1}$ norm, the Manhattan distance. \n",
    "\n",
    "In the example above the distance between $\\mathbf{x}=\\left[\\begin{array}{c}\n",
    "1\\\\\n",
    "2\n",
    "\\end{array}\\right]$ and $\\mathbf{y}=\\left[\\begin{array}{c}\n",
    "9\\\\\n",
    "8\n",
    "\\end{array}\\right]$ using the $\\ell_{1}$ norm is calculated as $\\left|1-9\\right|+\\left|2-8\\right|=14$. \n",
    "\n",
    "\\section{The $\\ell_{\\infty}$ norm }\n",
    "\n",
    "The $\\ell_{\\infty}$ norm of a vector $\\mathbf{x}$ is equal to its\n",
    "largest entry in terms of absolute value, defined mathematically as \n",
    "\n",
    "\\begin{equation}\n",
    "\\Vert\\mathbf{x}\\Vert_{\\infty}=\\underset{n}{\\text{max}}\\left|x_{n}\\right|\n",
    "\\end{equation}\n",
    "\n",
    "For example, the distance between $\\mathbf{x}=\\left[\\begin{array}{c}\n",
    "1\\\\\n",
    "2\n",
    "\\end{array}\\right]$ and $\\mathbf{y}=\\left[\\begin{array}{c}\n",
    "9\\\\\n",
    "8\n",
    "\\end{array}\\right]$ in terms of the $\\ell_{\\infty}$ norm is found as $\\underset{}{\\text{max}}\\left(\\left|1-9\\right|,\\left|2-8\\right|\\right)$\n",
    "$=8$. \n",
    "\n",
    "\\section{What do these vector norms have in common?}\n",
    "\n",
    "The $\\ell_{2}$, $\\ell_{1}$, and $\\ell_{\\infty}$ norms, by virtue\n",
    "of being vector norms, share a number of useful properties that we\n",
    "detail below. Since these properties hold in general for {*}any{*}\n",
    "norm, we momentarily drop the subscript and represent the norm of\n",
    "$\\mathbf{x}$ simply by $\\Vert\\mathbf{x}\\Vert$.\n",
    "\n",
    "1. Norms are always non-negative, that is $\\Vert\\mathbf{x}\\Vert\\geq0$.\n",
    "Furthermore, the equality holds only when $\\mathbf{x}=\\mathbf{0}$.\n",
    "Therefore the norm of any nonzero vector is always greater than zero. \n",
    "\n",
    "2. The norm of $\\alpha\\mathbf{x}$, that is a scalar multiple of $\\mathbf{x}$,\n",
    "can be written in terms of the norm of $\\mathbf{x}$, as $\\Vert\\alpha\\mathbf{x}\\Vert=\\left|\\alpha\\right|\\Vert\\mathbf{x}\\Vert$.\n",
    "With $\\alpha=-1$ for example, we have that $\\Vert-\\mathbf{x}\\Vert=\\Vert\\mathbf{x}\\Vert$. \n",
    "\n",
    "3. Norms also satisfy the so-called triangle inequality where for\n",
    "any vectors $\\mathbf{x}$, $\\mathbf{y}$, and $\\mathbf{z}$ we have\n",
    "$\\Vert\\mathbf{x}-\\mathbf{z}\\Vert+\\Vert\\mathbf{z}-\\mathbf{y}\\Vert\\geq\\Vert\\mathbf{x}-\\mathbf{y}\\Vert$.\n",
    "As illustrated in the figure below for the $\\ell_{2}$ norm (left)\n",
    "and the $\\ell_{1}$ norm (right), the triangle inequality simply states\n",
    "that the distance between $\\mathbf{x}$ and $\\mathbf{y}$ is always\n",
    "smaller than (or equal to) the distance between $\\mathbf{x}$ and\n",
    "$\\mathbf{z}$, and the distance between $\\mathbf{z}$ and $\\mathbf{y}$\n",
    "combined. In other words, if one wanted to travel from a given point\n",
    "$\\mathbf{x}$ to a given point $\\mathbf{y}$, it would be always better\n",
    "to travel directly from $\\mathbf{x}$ to $\\mathbf{y}$, than to travel\n",
    "first to a third point z, and then to y. With the change of variables\n",
    "$\\mathbf{u}=\\mathbf{x}-\\mathbf{z}$ and $\\mathbf{v}=\\mathbf{z}-\\mathbf{y}$,\n",
    "the triangle inequality is sometimes written in the simpler form of\n",
    "$\\Vert\\mathbf{u}\\Vert+\\Vert\\mathbf{v}\\Vert\\geq\\Vert\\mathbf{u}+\\mathbf{v}\\Vert$\n",
    "for all vectors $\\mathbf{u}$ and $\\mathbf{v}$.\n",
    "\n",
    "In addition to the general properties mentioned above and held by\n",
    "any norm, the $\\ell_{2}$, $\\ell_{1}$, and $\\ell_{\\infty}$ norms\n",
    "share a stronger bond that ties them together: they are all members\n",
    "of the $\\ell_{p}$ norm family. The $\\ell_{p}$ norm is generally\n",
    "defined as \n",
    "\n",
    "\\begin{equation}\n",
    "\\left\\Vert \\mathbf{x}\\right\\Vert _{p}=\\left(\\sideset{}{_{n=1}^{N}}\\sum\\left|x_{n}\\right|^{p}\\right)^{\\frac{1}{p}}\\label{eq:lp norm-1}\n",
    "\\end{equation}\n",
    "\n",
    "for $p\\geq1$. One can easily verify that with $p=1$, $p=2$, and\n",
    "as $p\\longrightarrow\\infty$, the $\\ell_{p}$ norm reduces to the\n",
    "$\\ell_{1}$, $\\ell_{2}$, and $\\ell_{\\infty}$ norm respectively. \n",
    "\n",
    "\\section{The $\\ell_{p}$ norm balls }\n",
    "\n",
    "A norm ball is a set of all vectors $\\mathbf{x}$ with same norm value,\n",
    "that is, all $\\mathbf{x}$such that $\\left\\Vert \\mathbf{x}\\right\\Vert =c$\n",
    "for some constant $c>0$. When $c=1$, this set is called the unit\n",
    "norm ball, or simply the unit ball. In the Python cell below we show\n",
    "the $\\ell_{p}$ unit balls in two dimensions for different values\n",
    "of $p$.\n",
    "\n",
    "As you can see above, the $\\ell_{1}$ unit ball takes the form of\n",
    "a dimaond, whose equation is given by\n",
    "\n",
    "\\begin{equation}\n",
    "\\left|x_{1}\\right|+\\left|x_{2}\\right|=1\\label{eq:lp norm-1-1-1}\n",
    "\\end{equation}\n",
    "\n",
    "The $\\ell_{2}$ unit ball is a cricle defined by \n",
    "\n",
    "\\begin{equation}\n",
    "x_{1}^{2}+x_{2}^{2}=1\\label{eq:lp norm-1-1}\n",
    "\\end{equation}\n",
    "\n",
    "As $p\\longrightarrow\\infty$ the unit ball approaches a square characterized\n",
    "by\n",
    "\n",
    "\\begin{equation}\n",
    "\\underset{}{\\text{max}}\\left(\\left|x_{1}\\right|,\\left|x_{2}\\right|\\right)=1.\\label{eq:lp norm-1-1-2}\n",
    "\\end{equation}\n",
    "\n",
    "\\section{Matrix norms}\n",
    "\n",
    "\\section{The Frobenius norm}\n",
    "\n",
    "Recall that the $\\ell_{2}$ norm of a vector is defined as the square\n",
    "root of the sum of the squares of its elements. The Frobenuis norm\n",
    "is the intuitive extension of the $\\ell_{2}$ norm for vectors to\n",
    "matrices, defined similarly as the square root of the sum of the squares\n",
    "of all the elements in the matrix. Thus, the Frobenuis norm of an\n",
    "$N\\times M$ dimensional matrix $\\mathbf{X}$ is calculated as\n",
    "\n",
    "\\begin{equation}\n",
    "\\left\\Vert \\mathbf{X}\\right\\Vert _{F}=\\sqrt{\\sideset{}{_{n=1}^{N}}\\sum\\sideset{}{_{m=1}^{M}}\\sum x_{n,m}^{2}}\n",
    "\\end{equation}\n",
    "\n",
    "For example, the Frobenuis norm of the matrix $\\mathbf{X}=\\left[\\begin{array}{cc}\n",
    "-1 & 2\\\\\n",
    "0 & 5\n",
    "\\end{array}\\right]$ can be found as $\\sqrt{\\left(-1\\right)^{2}+2^{2}+0^{2}+5^{2}}=\\sqrt{30}$.\n",
    "\n",
    "The connection between the $\\ell_{2}$ norm and the Frobenuis norm\n",
    "goes even deeper: collecting all singular values of $\\mathbf{X}$\n",
    "in the vector $\\mathbf{s}$ (assuming $N\\leq M$)\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{s}=\\left[\\begin{array}{c}\n",
    "\\sigma_{1}\\\\\n",
    "\\sigma_{2}\\\\\n",
    "\\vdots\\\\\n",
    "\\sigma_{N}\n",
    "\\end{array}\\right]\n",
    "\\end{equation}\n",
    "\n",
    "The Frobenuis norm of $\\mathbf{X}$ can be shown to be equal to the\n",
    "$\\ell_{2}$ norm of $\\mathbf{s}$, i.e.,\n",
    "\n",
    "\\begin{equation}\n",
    "\\left\\Vert \\mathbf{X}\\right\\Vert _{F}=\\left\\Vert \\mathbf{s}\\right\\Vert _{2}\n",
    "\\end{equation}\n",
    "\n",
    "\\section{The spectral and nuclear norms}\n",
    "\n",
    "The observation that the $\\ell_{2}$ norm of the vector of sigular\n",
    "values of a matrix is identical to its Frobenuis norm motivates the\n",
    "use of other $\\ell_{p}$ norms on the vector $\\mathbf{s}$. In particular\n",
    "the $\\ell_{1}$ norm of $\\mathbf{s}$ defines the nuclear norm of\n",
    "$\\mathbf{X}$ denoted by $\\left\\Vert \\mathbf{X}\\right\\Vert _{*}$ \n",
    "\n",
    "\\begin{equation}\n",
    "\\left\\Vert \\mathbf{X}\\right\\Vert _{*}=\\left\\Vert \\mathbf{s}\\right\\Vert _{1}\n",
    "\\end{equation}\n",
    "\n",
    "and the $\\ell_{\\infty}$ norm of $\\mathbf{s}$ defines the spectral\n",
    "norm of $\\mathbf{X}$, denoted by $\\left\\Vert \\mathbf{X}\\right\\Vert _{2}$\n",
    "\n",
    "\\begin{equation}\n",
    "\\left\\Vert \\mathbf{X}\\right\\Vert _{2}=\\left\\Vert \\mathbf{s}\\right\\Vert _{\\infty}\n",
    "\\end{equation}\n",
    "\n",
    "Because the singular values of real matrices are always non-negative,\n",
    "the spectral norm and the nuclear norm are simply the largest and\n",
    "the sum of the singular values, respectively. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "121px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
