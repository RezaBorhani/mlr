{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Unsupervised Learning Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Part 2:  K-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post we discuss another archetypal unsupervised learning problem called clustering. In general clustering is done for one of two reasons: \n",
    "\n",
    "**(1) As a pre-processing step and to reduce the dimensionality of very large datasets:** today's datasets can be extremely large in terms of number of input data points. By clustering this very large number of data points into a relatively small number of clusters and representing all members of each cluster by that cluster's centroid, we can siginificantly reduce the computational cost of applying more complex machine learning algorithms to very large datasets. \n",
    "\n",
    "**(2) To uncover underlyig structures in the data:** Sometimes clustering is done not as a pre-processing step in the machine learning pipeline, but to understand the underlying structures in the data. For example, clustering methods have been applied to genetic data in an attempt to help simplify this sort of data for human interpretation [d2005does, eisen1998cluster, ben1999clustering]. This is important because while there are many genes whose cellular functionalities are well understood by geneticists, the functionality of most remain a mystery. Clustering gene expression data together with the knowledge of the functionality of some members of a gene cluster then give a new insight into the potential functionality of the rest.\n",
    "\n",
    "In this post we focus on the K-means --> fundamental -- like PCA  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large datasets, as well as data consisting of a large number of features, present computational problems in the training of predictive models. In this Chapter we discuss several useful techniques for reducing the dimension of a given dataset, that is reducing the number of data points or number of features, often employed in order to make predictive learning methods scale to larger datasets. More specifically, in this Chapter we discuss widely used methods for reducing the data dimension, that is the number of data points, of a dataset including random subsampling and K-means clustering. We then detail a common way of reducing the feature dimension, or number features, of a dataset as explained pictorially in Figure [fig:Abstract-summary-dimension-reduction-techniques]. A classical approach for feature dimension reduction, Principal Component Analysis (PCA) while often used for general data analysis is a relatively poor tool for reducing the feature dimension of predictive modeling data. However PCA presents a fundamental mathematical archetype, the matrix factorization, that provides a very useful way of organizing our thinking about a wide array of important learning models (including linear regression, K-means, Recommender Systems - introduced after detailing PCA in this Chapter), all of which may be thought of as variations of the simple theme of matrix factorization. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports from custom library\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "import matplotlib.pyplot as plt\n",
    "import autograd.numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "%matplotlib notebook\n",
    "\n",
    "# this is needed to compensate for %matplotlib notebook's tendancy to blow up images when plotted inline\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.autolayout'] = True\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. K-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With K-means clustering we reduce the data dimension by finding suitable representatives or centroids for clusters, or groups, of data points. All members of each cluster are then represented by their cluster's respective centroid. Hence the problem of clustering is that of partitioning data into clusters of points with similar characteristics, and with K-means specifically this characteristic is geometric closeness in the feature space.\n",
    "\n",
    "Although it is possible to adopt a variety of different ways to define similarity/closeness between data points in the feature space (e.g., spectral clustering and subspace clustering), proximity in the Euclidean sense is the most popular measure for clustering data.\n",
    "\n",
    "The figure below illustrates K-means clustering performed on a 2-D toy dataset with $P = 10$ data points, where in the right panel data points are clustered into $K = 3$ clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src= '../../mlrefined_images/unsupervised_images/Fig_9_2.png' width=\"60%\" height=\"auto\" alt=\"\"/>\n",
    "  <figcaption>   \n",
    "<strong>Figure 1:</strong> <em> (left) A 2-D toy dataset with $P=10$ data points. (right) Original data clustered into $K=3$ clusters where each cluster centroid is marked by a '$\\times$' symbol. Points that are geometrically close to one another belong to the same cluster. </em>  </figcaption> \n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Notation and modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With K-means we look to partition $P$ data points each of dimension $N$, into $K$ clusters and find a representative centroid denoted for each cluster. For the moment we will assume that we know the location of these $K$ cluster centroids, as illustrated figuratively in the toy example in the right panel of Figure 1, in order to derive formally the desired relationship between the data and centroids. Once this is expressed clearly we will use it in order to form a learning problem for the accurate recovery of cluster centroids, dropping the unrealistic notion that we have pre-conceived knowledge of their location. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denoting by $\\mathbf{c}_{k}$ the centroid of the $k^{th}$ cluster and $\\mathcal{S}_{k}$ the set of indices of the subset of those $P$ data points, denoted $\\mathbf{x}_{1}, \\dots, \\mathbf{x}_{P}$, belonging to this cluster the desire that points in the $k^{th}$ cluster should lie close to its centroid may be written mathematically as\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{c}_{k}\\approx\\mathbf{x}_{p}\\quad\\textrm{for all }p\\in\\mathcal{S}_{k}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for all $k=1, \\ldots, K$. These desired relations can be written more conveniently by first stacking the centroids column-wise into the *centroid matrix* \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{C}=\\left[\\begin{array}{cccc}\n",
    "\\mathbf{c}_{1} & \\mathbf{c}_{2} & \\cdots & \\mathbf{c}_{K}\\end{array}\\right]\n",
    "\\end{equation}\n",
    "\n",
    "Then, denoting by $\\mathbf{e}_{k}$ the $k^{th}$ standard basis vector (that is a $K\\times1$ vector with a $1$ in the $k^{th}$ slot and zeros elsewhere) we may write $\\mathbf{c}_{k}$ as $\\mathbf{C}\\,\\mathbf{e}_{k}$, and hence the relations in equation (1) may be written equivalently for each $k$ as\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{C} \\, \\mathbf{e}_{k}\\approx\\mathbf{x}_{p}\\quad\\textrm{for all }p\\in\\mathcal{S}_{k}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, to write these equations even more conveniently we stack the data column-wise into the data matrix\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{X}=\\left[\\begin{array}{cccc}\n",
    "\\mathbf{x}_{1} & \\mathbf{x}_{2} & \\cdots & \\mathbf{x}_{P}\\end{array}\\right]\n",
    "\\end{equation}\n",
    "\n",
    "and form a $K\\times P$ *assignment matrix* $\\mathbf{W}$. The $p^{th}$ column of this matrix, denoted as $\\mathbf{w}_{p}$, is the standard basis vector associated with the cluster to which the $p^{th}$ point belongs, i.e., $\\mathbf{w}_{p}=\\mathbf{e}_{k}$ if $p\\in\\mathcal{S}_{k}$. With this $\\mathbf{w}_{p}$ notation we may write each equation in (3) as $\\mathbf{C}\\mathbf{w}_{p}\\approx\\mathbf{x}_{p}$ for all $p\\in\\mathcal{S}_{k}$, or using matrix notation all $K$ such relations simultaneously as\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{CW}\\approx\\mathbf{X}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Figure 2 pictorially illustrates the compactly written desired K-means relationship in (5) for the dataset shown in Figure 1. Note that the location of the only nonzero entry in each column of the assignment matrix $\\mathbf{W}$ determines the cluster membership of its corresponding data point in $\\mathbf{X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<figure>\n",
    "  <img src= '../../mlrefined_images/unsupervised_images/Fig_9_3.png' width=\"60%\" height=\"auto\" alt=\"\"/>\n",
    "  <figcaption>   \n",
    "<strong>Figure 2:</strong> <em> K-means clustering relations described in a compact matrix form. Cluster centroids in $C$ lie close to their corresponding cluster points in $X$. The $p^{th}$ column of the assignment matrix $W$ contains the standard basis vector corresponding to the data point's cluster centroid. </em>  </figcaption> \n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may now drop the assumption that we know the locations of cluster centroids and the knowledge of which points are assigned to them, i.e., the exact description of the centroid matrix $\\mathbf{C}$ and assignment matrix $\\mathbf{W}$. We want to *learn* the right values for these two matrices. Specifically, we know that the ideal $\\mathbf{C}$ and $\\mathbf{W}$ satisfy the compact relations described in equation (5), i.e., that $\\mathbf{C}\\mathbf{W}\\approx\\mathbf{X}$ or in other words that $\\left\\Vert \\mathbf{C}\\mathbf{W}-\\mathbf{X}\\right\\Vert _{F}^{2}$ is small, while $\\mathbf{W}$ consists of properly chosen standard basis vectors relating the data points to their respective centroids. Thus we phrase a K-means optimization problem whose solution precisely satisfies these requirements as\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\\underset{\\mathbf{C},\\mathbf{W}}{\\mbox{minimize}} & \\,\\,\\left\\Vert \\mathbf{CW}-\\mathbf{X}\\right\\Vert _{F}^{2}\\\\\n",
    "\\mbox{subject to} & \\,\\,\\,\\,\\,\\mathbf{w}_{p}\\in\\left\\{ \\mathbf{e}_{k}\\right\\} _{k=1}^{K},\\,\\,\\, p=1,\\ldots,P\n",
    "\\end{aligned}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, how the K-means problem given in equation (6) is - structurally - just a matrix factorization problem like PCA with additional set of constraints on the weight matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Optimization of the K-means problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Akin to PCA, the K-means' objective is non-convex, and because we cannot minimize over both $\\mathbf{C}$ and $\\mathbf{W}$ simultaneously, it is solved via alternating minimization, that is by alternately minimizing the objective function in (6) over one of the variables ($\\mathbf{C}$ or $\\mathbf{W}$), while keeping the other variable fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over $\\mathbf{W}$ the K-means problem in (6) reduces to\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\\underset{\\mathbf{W}}{\\mbox{minimize}} & \\,\\,\\left\\Vert \\mathbf{CW}-\\mathbf{X}\\right\\Vert _{F}^{2}\\\\\n",
    "\\mbox{subject to} & \\,\\,\\,\\,\\,\\mathbf{w}_{p}\\in\\left\\{ \\mathbf{e}_{k}\\right\\} _{k=1}^{K},\\,\\,\\, p=1,\\ldots,P\n",
    "\\end{aligned}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noting that the objective in (7) can be equivalently written as $\\sum_{p=1}^{P}\\left\\Vert \\mathbf{Cw}_{p}-\\mathbf{x}_{p}\\right\\Vert _{2}^{2}$ (again $\\mathbf{C}$ is fixed) and that each $\\mathbf{w}_{p}$ appears in only one summand, we can recover each column $\\mathbf{w}_{p}$ independently by solving\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\\underset{\\mathbf{\\mathbf{w}}_{p}}{\\mbox{minimize}} & \\,\\,\\left\\Vert \\mathbf{Cw}_{p}-\\mathbf{x}_{p}\\right\\Vert _{2}^{2}\\\\\n",
    "\\mbox{subject to} & \\,\\,\\mathbf{w}_{p}\\in\\left\\{ \\mathbf{e}_{k}\\right\\} _{k=1}^{K}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "for each $p=1,\\ldots,P$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is precisely the problem of assigning a data point $\\mathbf{x}_{p}$ to its closest centroid, i.e., finding $k$ such that $\\left\\Vert \\mathbf{c}_{k}-\\mathbf{x}_{p}\\right\\Vert _{2}^{2}$ is smallest! We can see that this is precisely the problem above by unraveling our compact notation: given the constraint on $\\mathbf{w}_{p}$, we have $\\mathbf{Cw}_{p}=\\mathbf{c}_{k}$ whenever $\\mathbf{w}_{p}=\\mathbf{e}_{k}$ and so the objective may be written as $\\left\\Vert \\mathbf{Cw}_{p}-\\mathbf{x}_{p}\\right\\Vert _{2}^{2}=\\left\\Vert \\mathbf{c}_{k}-\\mathbf{x}_{p}\\right\\Vert _{2}^{2}$. Hence the problem of finding $\\mathbf{w}_{p}$, or finding the closest centroid to $\\mathbf{x}_{p}$, may be written as\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\\underset{k=1,\\dots,K}{\\mbox{minimize}} & \\,\\,\\left\\Vert \\mathbf{c}_{k}-\\mathbf{x}_{p}\\right\\Vert _{2}^{2}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "which can be solved by simply computing the objective for each $k$ and finding the smallest value. Then for whichever $k^{\\star}$ minimizes the above we may set $\\mathbf{w}_{p}=\\mathbf{e}_{k^{\\star}}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now minimizing (6) over $\\mathbf{C}$, we have no constraints (they being applied only to $\\mathbf{W}$) and have the problem\n",
    "\n",
    "\\begin{equation}\n",
    "\\underset{\\mathbf{C}}{\\mbox{minimize}}\\,\\,\\left\\Vert \\mathbf{CW}-\\mathbf{X}\\right\\Vert _{F}^{2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we may use the first order condition: setting the derivative of $g\\left(\\mathbf{C}\\right)=\\left\\Vert \\mathbf{CW}-\\mathbf{X}\\right\\Vert _{F}^{2}$ to zero gives the linear system\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{C}\\mathbf{W}\\mathbf{W}^{T}=\\mathbf{X}\\mathbf{W}^{T}\n",
    "\\end{equation}\n",
    "\n",
    "for the optimal $\\mathbf{C}$ denoted as $\\mathbf{C}^{\\star}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easy to verify that $\\mathbf{W}\\mathbf{W}^{T}$ is a $K\\times K$ diagonal matrix whose $k^{th}$ diagonal entry is equal to the number of data points assigned to the $k^{th}$ cluster, and that the $k^{th}$ column of $\\mathbf{X}\\mathbf{W}^{T}$ is the sum of all data points in the $k^{th}$ cluster. Hence each column of $\\mathbf{C}^{\\star}$ can therefore be calculated independently as \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{c}_{k}^{\\star}=\\frac{1}{\\left|\\mathcal{S}_{k}\\right|}\\underset{p\\in\\mathcal{S}_{k}}{\\sum}\\mathbf{x}_{p}\\,\\,\\,\\forall k\n",
    "\\end{equation}\n",
    "\n",
    "In other words, $\\mathbf{c}_{k}^{\\star}$ is the average of all data points in the $k^{th}$ cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we repeat these alternating updates for a maximum number of iterations or until neither matrix changes too much from iteration to iteration. For convenience we present the resulting algorithm below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The K-means algorithm\n",
    "\n",
    "<hr style=\"height:1px;border:none;color:#555;background-color:#555;\">\n",
    "<p style=\"line-height: 1.7;\">\n",
    "<strong>1:</strong>&nbsp;&nbsp; <strong>input:</strong> $N \\times P$ data matrix $\\mathbf{X}$, initialized $N \\times K$ centroid matrix $\\mathbf{C}$, and maximum number of iterations $J$ <br>\n",
    "\n",
    "<strong>2:</strong>&nbsp;&nbsp; <code>for</code> $\\,\\,i = 1,\\ldots,J$<br>\n",
    "\n",
    "<strong>3:</strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <code>for</code> $\\,\\,p = 1,\\ldots,P$<br>\n",
    "\n",
    "<strong>4:</strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $k^{\\star}=\\underset{k=1,\\ldots,K}{\\mbox{argmin}}\\,\\,\\left\\Vert \\mathbf{c}_{k}-\\mathbf{x}_{p}\\right\\Vert _{2}^{2}$<br>\n",
    "\n",
    "<strong>5:</strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; update $\\mathbf{w}_p$ via $\\mathbf{w}_{p}=\\mathbf{e}_{k^{\\star}}$ where $\\mathbf{e}_{k}$ is the $k^{th}$ standard basis vector<br>\n",
    "\n",
    "<strong>6:</strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <code>end for</code><br>\n",
    "\n",
    "<strong>7:</strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <code>for</code> $\\,\\,k = 1,\\ldots,K$<br>\n",
    "\n",
    "<strong>8:</strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; denote $\\mathcal{S}_{k}$ the index set of points $\\mathbf{x}_{p}$ currently assigned to the $k^{th}$ cluster<br>\n",
    "\n",
    "<strong>9:</strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; update $\\mathbf{c}_k$ via $\\mathbf{c}_{k}=\\frac{1}{\\left|\\mathcal{S}_{k}\\right|}\\underset{p\\in\\mathcal{S}_{k}}{\\sum}\\mathbf{x}_{p}$<br>\n",
    "\n",
    "<strong>10:</strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <code>end for</code><br>\n",
    "\n",
    "<strong>11:</strong>&nbsp; <code>end for</code><br>\n",
    "\n",
    "<strong>12:</strong>&nbsp; <strong>output:</strong> optimal centroid matrix $\\mathbf{C}$ and assignment matrix $\\mathbf{W}$<br>\n",
    "\n",
    "<hr style=\"height:1px;border:none;color:#555;background-color:#555;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#a50e3e;\">Example 1: </span> Initialization of K-means in consideration of its non-convexity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the K-means cost function is non-convex it is possible for the alternating minimization procedure described above to find non-global minima of the objective function. As with all non-convex problems, this depends on the initializations of our optimization variables (or in this instance just the $\\mathbf{C}$ matrix initialization since the procedure begins by updating it independently of $\\mathbf{W}$). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of the algorithm reaching poor minima can have significant impact on the quality of the clusters learned. For example, in Figure 3 below we use a 2-D toy dataset with $K = 2$ clusters. With the initial centroid positions shown in the top panel, the K-means algorithm gets stuck in a local minimum and consequently fails to cluster the data properly. A different initialization for one of the centroids, however, leads to a successful clustering of the data, as shown in the lower panel of Figure 3. To overcome the issue of non-convexity of K-means in practice we usually run the algorithm multiple times with different initializations, seeking out the lowest possible minimum of the objective, and the solution resulting in the smallest value of the objective function is selected as the final solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src= '../../mlrefined_images/unsupervised_images/Fig_9_4.png' width=\"60%\" height=\"auto\" alt=\"\"/>\n",
    "  <figcaption>   \n",
    "<strong>Figure 3:</strong> <em> Success or failure of K-means depends on the centroids' initialization. (top) (i) two centroids are initialized, (ii) cluster assignment is updated, (iii) centroid locations are updated, (iv) no change in the cluster assignment of the data points leads to stopping of the algorithm. (below) (i) two centroids are initialized with the red one being initialized differently, (ii) cluster assignment is updated, (iii) centroid locations are updated, (iv) cluster assignment is updated, (v) centroid locations are updated, (vi) no change in the cluster assignment of the data points leads to stopping of the algorithm. </em>  </figcaption> \n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "214px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 1,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
