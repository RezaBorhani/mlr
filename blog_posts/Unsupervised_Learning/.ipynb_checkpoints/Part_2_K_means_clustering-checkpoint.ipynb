{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Unsupervised Learning Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Part 2:  K-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post we discuss another archetypal unsupervised learning problem called clustering. In general clustering is done for one of two reasons: \n",
    "\n",
    "**(1) As a pre-processing step and to reduce the dimensionality of very large datasets:** today's datasets can be extremely large in terms of number of input data points. By clustering this very large number of data points into a relatively small number of clusters and representing all members of each cluster by that cluster's centroid, we can siginificantly reduce the computational cost of applying more complex machine learning algorithms to very large datasets. \n",
    "\n",
    "**(2) To uncover underlyig structures in the data:** Sometimes clustering is done not as a pre-processing step in the machine learning pipeline, but to understand the underlying structures in the data. For example, clustering methods have been applied to genetic data in an attempt to help simplify this sort of data for human interpretation [d2005does, eisen1998cluster, ben1999clustering]. This is important because while there are many genes whose cellular functionalities are well understood by geneticists, the functionality of most remain a mystery. Clustering gene expression data together with the knowledge of the functionality of some members of a gene cluster then give a new insight into the potential functionality of the rest.\n",
    "\n",
    "In this post we focus on the K-means --> fundamental -- like PCA  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports from custom library\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "import matplotlib.pyplot as plt\n",
    "import autograd.numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "%matplotlib notebook\n",
    "\n",
    "# this is needed to compensate for %matplotlib notebook's tendancy to blow up images when plotted inline\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.autolayout'] = True\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. K-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With K-means clustering we reduce the data dimension by finding suitable representatives or centroids for clusters, or groups, of data points. All members of each cluster are then represented by their cluster's respective centroid. Hence the problem of clustering is that of partitioning data into clusters of points with similar characteristics, and with K-means specifically this characteristic is geometric closeness in the feature space.\n",
    "\n",
    "Although it is possible to adopt a variety of different ways to define similarity/closeness between data points in the feature space (e.g., spectral clustering and subspace clustering), proximity in the Euclidean sense is the most popular measure for clustering data.\n",
    "\n",
    "The figure below illustrates K-means clustering performed on a 2-D toy dataset with $P = 10$ data points, where in the right panel data points are clustered into $K = 3$ clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src= '../../mlrefined_images/unsupervised_images/Fig_9_2.png' width=\"60%\" height=\"auto\" alt=\"\"/>\n",
    "  <figcaption>   \n",
    "<strong>Figure 1:</strong> <em> (left) A 2-D toy dataset with $P=10$ data points. (right) Original data clustered into $K=3$ clusters where each cluster centroid is marked by a '$\\times$' symbol. Points that are geometrically close to one another belong to the same cluster. </em>  </figcaption> \n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Notation and modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With K-means we look to partition $P$ data points each of dimension $N$, into $K$ clusters and find a representative centroid denoted for each cluster. For the moment we will assume that we know the location of these $K$ cluster centroids, as illustrated figuratively in the toy example in the right panel of Figure 1, in order to derive formally the desired relationship between the data and centroids. Once this is expressed clearly we will use it in order to form a learning problem for the accurate recovery of cluster centroids, dropping the unrealistic notion that we have pre-conceived knowledge of their location. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denoting by $\\mathbf{c}_{k}$ the centroid of the $k^{th}$ cluster and $\\mathcal{S}_{k}$ the set of indices of the subset of those $P$ data points, denoted $\\mathbf{x}_{1}, \\dots, \\mathbf{x}_{P}$, belonging to this cluster the desire that points in the $k^{th}$ cluster should lie close to its centroid may be written mathematically as\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{c}_{k}\\approx\\mathbf{x}_{p}\\quad\\textrm{for all }p\\in\\mathcal{S}_{k}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for all $k=1, \\ldots, K$. These desired relations can be written more conveniently by first stacking the centroids column-wise into the *centroid matrix* \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{C}=\\left[\\begin{array}{cccc}\n",
    "\\mathbf{c}_{1} & \\mathbf{c}_{2} & \\cdots & \\mathbf{c}_{K}\\end{array}\\right]\n",
    "\\end{equation}\n",
    "\n",
    "Then, denoting by $\\mathbf{e}_{k}$ the $k^{th}$ standard basis vector (that is a $K\\times1$ vector with a $1$ in the $k^{th}$ slot and zeros elsewhere) we may write $\\mathbf{c}_{k}$ as $\\mathbf{C}\\,\\mathbf{e}_{k}$, and hence the relations in equation (1) may be written equivalently for each $k$ as\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{C} \\, \\mathbf{e}_{k}\\approx\\mathbf{x}_{p}\\quad\\textrm{for all }p\\in\\mathcal{S}_{k}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, to write these equations even more conveniently we stack the data column-wise into the data matrix\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{X}=\\left[\\begin{array}{cccc}\n",
    "\\mathbf{x}_{1} & \\mathbf{x}_{2} & \\cdots & \\mathbf{x}_{P}\\end{array}\\right]\n",
    "\\end{equation}\n",
    "\n",
    "and form a $K\\times P$ *assignment matrix* $\\mathbf{W}$. The $p^{th}$ column of this matrix, denoted as $\\mathbf{w}_{p}$, is the standard basis vector associated with the cluster to which the $p^{th}$ point belongs, i.e., $\\mathbf{w}_{p}=\\mathbf{e}_{k}$ if $p\\in\\mathcal{S}_{k}$. With this $\\mathbf{w}_{p}$ notation we may write each equation in ([eq:slightly compact k-means]) as $\\mathbf{C}\\mathbf{w}_{p}\\approx\\mathbf{x}_{p}\\quad\\textrm{for all }p\\in\\mathcal{S}_{k}$, or using matrix notation all $K$ such relations simultaneously as\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{CW}\\approx\\mathbf{X}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, once $\\mathbf{C}$ and $\\mathbf{w}_{p}$ are learned the new $K$ dimensional feature representation of $\\mathbf{x}_{p}$ is then the vector $\\mathbf{w}_{p}$ (i.e., the weights over which $\\mathbf{x}_{p}$ is represented over the spanning set).\n",
    "\n",
    "By denoting \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{W}=\\left[\\begin{array}{cccc}\n",
    "\\mathbf{w}_{1} & \\mathbf{w}_{2} & \\cdots & \\mathbf{w}_{P}\\end{array}\\right]\n",
    "\\end{equation}\n",
    "\n",
    "the $K\\times P$ matrix of weights to learn, and \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{X}=\\left[\\begin{array}{cccc}\n",
    "\\mathbf{x}_{1} & \\mathbf{x}_{2} & \\cdots & \\mathbf{x}_{P}\\end{array}\\right]\n",
    "\\end{equation}\n",
    "\n",
    "the $N\\times P$ data matrix, all $P$ of these approximate equalities in equations (1) and (4) can be written compactly as\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{C}\\mathbf{W}\\approx\\mathbf{X}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of PCA, compactly stated in equation (7), naturally leads to determining $\\mathbf{C}$ and $\\mathbf{W}$ by minimizing $\\left\\Vert \\mathbf{C}\\mathbf{W}-\\mathbf{X}\\right\\Vert _{F}^{2}$, i.e., by solving\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\\underset{\\mathbf{C},\\mathbf{W}}{\\mbox{minimize}} & \\,\\,\\,\\,\\,\\left\\Vert \\mathbf{C}\\mathbf{W}-\\mathbf{X}\\right\\Vert _{F}^{2}\\end{aligned}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#a50e3e;\">Example 1: </span> PCA on simulated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Example we show the result of applying PCA on two simple 2D datasets using a solution to the PCA problem in (8) that we derive in Subsection 1.2.\n",
    "\n",
    "In the top panels dimension reduction via PCA retains much of the structure of the original data. Conversely, the more structured square data set loses much of its original characteristic after projection onto the PCA subspace as shown in the bottom panels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src= '../../mlrefined_images/unsupervised_images/Fig_9_6.png' width=\"55%\" height=\"auto\" alt=\"\"/>\n",
    "  <figcaption>   \n",
    "<strong>Figure 2:</strong> <em> (top panels) A simple 2-D data set (left, in blue) where dimension reduction via PCA retains much of the structure of the original data. The ideal subspace found via solving the PCA problem in (8) is shown in black in the left panel, and the data projected onto this subspace is shown in blue on the right. (bottom panels) Conversely, the more structured square data set loses much of its original structure after projection onto the PCA subspace. </em>  </figcaption> \n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#a50e3e;\">Example 2: </span> PCA and classification data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While PCA can technically be used for preprocessing data in a predictive modeling scenario, it can cause severe problems in the case of classification. In this Example we illustrate feature space dimension reduction via PCA on a simulated two-class dataset where the two classes are linearly separable. Because the ideal one-dimensional subspace in this instance runs parallel to the longer length of each class, projecting the complete dataset onto it completely destroys the separability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src= '../../mlrefined_images/unsupervised_images/Fig_9_7.png' width=\"55%\" height=\"auto\" alt=\"\"/>\n",
    "  <figcaption>   \n",
    "<strong>Figure 3:</strong> <em> (left) A toy classification dataset consisting of two linearly separable classes. The ideal subspace produced via PCA is shown in black. (right) Projecting the data onto this subspace (in other words reducing the feature space dimension via PCA) destroys completely the original separability of the data.  </em>  </figcaption> \n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#a50e3e;\">Example 3: </span> Role of efficient bases in digital image compression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Digital image compression aims at reducing the size of digital images without adversely affecting their quality. Without compression a natural photo taken by a digital camera would require one to two orders of magnitude more storage space. As shown in the figure below, in a typical image compression scheme an input image is first cut up into small square (typically $8 \\times 8$ pixel) blocks. The values of pixels in each block (which are integers between $0$ and $255$ for an 8-bit grayscale image) are stacked into a column vector $y$, and compression is then performed on these individual vectorized blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src= '../../mlrefined_images/unsupervised_images/Fig_9_8.png' width=\"50%\" height=\"auto\" alt=\"\"/>\n",
    "  <figcaption>   \n",
    "<strong>Figure 4:</strong> <em> In a prototypical image compression scheme the input image is cut into $8 \\times 8$ blocks. Each block is then vectorized to make a $64 \\times 1$ column vector $y$ which will be input to the compression algorithm.\n",
    "  </em>  </figcaption> \n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary idea behind many digital image compression algorithms is that with the use of specific bases, we only need very few of their elements to very closely approximate any natural image. One such basis, the $8 \\times 8$ discrete cosine transform (DCT) which is the backbone of the popular JPEG compression scheme, consists of two-dimensional cosine waves of varying frequencies, and is shown in the figure below along with its analogue standard basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src= '../../mlrefined_images/unsupervised_images/Fig_9_9.png' width=\"50%\" height=\"auto\" alt=\"\"/>\n",
    "  <figcaption>   \n",
    "<strong>Figure 5:</strong> <em>(left) The set of $64$ DCT basis elements used for compression of $8 \\times 8$ image blocks. For visualization purposes pixel values in each basis patch are gray-coded so that white and black colors correspond to the minimum and maximum value in that patch, respectively. (right) The set of $64$ standard basis elements, each having only one nonzero entry. Pixel values are gray-coded so that white and black colors correspond to entry values of $1$ and $0$, respectively. Most natural image blocks can be approximated as a linear combination of just a few DCT basis elements while the same cannot be said of the standard basis.\n",
    " </em>  </figcaption> \n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most natural image blocks can be well approximated using only a few elements of the DCT basis. The reason is, as opposed to bases with more locally defined elements (e.g., the standard basis), each DCT basis element represents a fluctuation commonly seen across the entirety of a natural image block. Therefore with just a few of these elements, properly weighted, we can approximate a wide range of image blocks. In other words, instead of seeking out a basis (as with PCA), here we have a fixed basis over which image data can be very efficiently represented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform compression, DCT basis patches in Figure 5 are vectorized into a sequence of $P=64$ fixed basis vectors $\\left\\{ \\mathbf{c}_{p}\\right\\} _{p=1}^{P}$ in the same manner as the input image blocks. Concatenating these patches column-wise into a matrix $\\mathbf{C}$ and supposing there are $K$ blocks in the input image, denoting by $\\mathbf{x}_{k}$ its $k^{th}$ vectorized block, to represent the entire image over the basis we solve $K$ linear systems of equations of the form\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{C}\\mathbf{w}_{k}=\\mathbf{x}_{k}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each vector $\\mathbf{w}_{k}$ in equation (9) stores the DCT coefficients (or weights) corresponding to the image block $\\mathbf{x}_{k}$. Most of the weights in the coefficient vectors $\\left\\{ \\mathbf{w}_{k}\\right\\} _{k=1}^{K}$ are typically quite small. Therefore, as illustrated by an example image in Figure 6 below, setting even $80\\%$ of the smallest weights to zero gives an approximation that is essentially indistinguishable from the original image. Even setting $99\\%$ of the smallest weights to zero gives an approximation to the original data wherein we can still identify the objects in the original image. To compress the image, instead of storing each pixel value, only these few remaining nonzero coefficients are kept. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src= '../../mlrefined_images/unsupervised_images/Fig_9_10.png' width=\"90%\" height=\"auto\" alt=\"\"/>\n",
    "  <figcaption>   \n",
    "<strong>Figure 6:</strong> <em> From left to right, the original $256 \\times 256$ input image along with its three compressed versions where we keep only the largest $20\\%$, $5\\%$, and $1\\%$ of the DCT coefficients to represent the image, resulting in compression by a factor of $5$, $20$, and $100$, respectively. Although, as expected, the visual quality deteriorates as the compression factor increases, the $1\\%$ image still captures a considerable amount of information. This example is a testament to the efficiency of DCT basis in representing natural image data.\n",
    " </em>  </figcaption> \n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Optimization of the PCA problem "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the PCA optimization problem in (8) is convex in each variable $\\mathbf{C}$ and $\\mathbf{W}$ individually (but non-convex in both simultaneously) a natural approach to solving this problem is to alternately minimize (8) over $\\mathbf{C}$ and $\\mathbf{W}$ independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beginning at an initial value for $\\mathbf{C}^{\\left(0\\right)}$ this produces a sequence of iterates $\\left(\\mathbf{W}^{\\left(i\\right)},\\,\\mathbf{C}^{\\left(i\\right)}\\right)$ where\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{array}{c}\n",
    "\\begin{aligned}\\mathbf{W}^{\\left(i\\right)}=\\,\\, & \\underset{\\mathbf{W}}{\\mbox{argmin}}\\,\\,\\left\\Vert \\mathbf{C}^{\\left(i-1\\right)}\\mathbf{W}-\\mathbf{X}\\right\\Vert _{F}^{2}\\end{aligned}\n",
    "\\\\\n",
    "\\begin{aligned}\\,\\,\\,\\,\\,\\mathbf{C}^{\\left(i\\right)}=\\,\\, & \\underset{\\mathbf{C}}{\\mbox{argmin}}\\,\\,\\left\\Vert \\mathbf{C}\\mathbf{W}^{\\left(i\\right)}-\\mathbf{X}\\right\\Vert _{F}^{2}\\end{aligned}\n",
    "\\end{array}\n",
    "\\end{equation}\n",
    "\n",
    "and where each may be expressed in closed form.\n",
    "\n",
    "Setting the gradient in each case to zero and solving gives\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{W}^{\\left(i\\right)}=\\left(\\left(\\mathbf{C}^{\\left(i-1\\right)}\\right)^{T}\\mathbf{C}^{\\left(i-1\\right)}\\right)^{\\dagger}\\left(\\mathbf{C}^{\\left(i-1\\right)}\\right)^{T}\\mathbf{X}\n",
    "\\end{equation}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{C}^{\\left(i\\right)}=\\mathbf{X}\\left(\\mathbf{W}^{\\left(i\\right)}\\right)^{T}\\left(\\mathbf{W}^{\\left(i\\right)}\\left(\\mathbf{W}^{\\left(i\\right)}\\right)^{T}\\right)^{\\dagger}\n",
    "\\end{equation}\n",
    "\n",
    "respectively, where $\\left(\\cdot\\right)^{\\dagger}$ denotes the pseudo-inverse. The procedure is stopped after taking a certain number of iterations and/or when the subsequent iterations do not change significantly.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternating minimization algorithm for PCA\n",
    "\n",
    "<hr style=\"height:1px;border:none;color:#555;background-color:#555;\">\n",
    "<p style=\"line-height: 1.7;\">\n",
    "<strong>1:</strong>&nbsp;&nbsp; <strong>Input:</strong> data matrix $\\mathbf{X}$, initial $\\mathbf{C}^{\\left(0\\right)}$, and maximum number of iterations $J$\n",
    "<br>\n",
    "\n",
    "<strong>2:</strong>&nbsp;&nbsp; <code>for</code> $\\,\\,i = 1,\\ldots,J$<br>\n",
    "\n",
    "<strong>3:</strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; update the weight matrix $\\mathbf{W}$ via $\\mathbf{W}^{\\left(i\\right)}=\\left(\\left(\\mathbf{C}^{\\left(i-1\\right)}\\right)^{T}\\mathbf{C}^{\\left(i-1\\right)}\\right)^{\\dagger}\\left(\\mathbf{C}^{\\left(i-1\\right)}\\right)^{T}\\mathbf{X}$<br>\n",
    "\n",
    "<strong>4:</strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; update the principal components matrix $\\mathbf{C}$ via $\\mathbf{C}^{\\left(i\\right)}=\\mathbf{X}\\left(\\mathbf{W}^{\\left(i\\right)}\\right)^{T}\\left(\\mathbf{W}^{\\left(i\\right)}\\left(\\mathbf{W}^{\\left(i\\right)}\\right)^{T}\\right)^{\\dagger}$<br>\n",
    "\n",
    "<strong>5:</strong>&nbsp;&nbsp; <code>end for</code><br>\n",
    "\n",
    "<strong>6:</strong>&nbsp;&nbsp; <strong>output:</strong> $\\mathbf{C}^{\\left(J\\right)}$ and $\\mathbf{W}^{\\left(J\\right)}$ <br>\n",
    "\n",
    "<hr style=\"height:1px;border:none;color:#555;background-color:#555;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it is not obvious at first sight, there is also a closed-form solution to the PCA problem in (8) based on the Singular Value Decomposition (SVD) of the matrix $\\mathbf{X}$. Denoting the SVD of $\\mathbf{X}$ as $\\mathbf{X}=\\mathbf{U}\\mathbf{S}\\mathbf{V}^{T}$ this solution is given as\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{array}{c}\n",
    "\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\mathbf{C}^{\\star}=\\mathbf{U}_{K}\\mathbf{S}_{K,K}\\\\\n",
    "\\mathbf{W}^{\\star}=\\mathbf{V}_{K}^{T}\n",
    "\\end{array}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\mathbf{U}_{K}$ and $\\mathbf{V}_{K}$ denote the matrices formed by the first $K$ columns of the left and right singular matrices $\\mathbf{U}$ and $\\mathbf{V}$ respectively, and $\\mathbf{S}_{K,K}$ denotes the upper $K\\times K$ sub-matrix of the singular value matrix $\\mathbf{S}$. Notice that since $\\mathbf{U}_{K}$ is an orthogonal matrix the recovered basis (for the low dimensional subspace) is indeed orthogonal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 PCA from the directional variance perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we show how to derive PCA from a different perspective, as the orthogonal directions of largest variance in data. While this perspective is not particularly useful when using PCA as a preprocessing technique (since all we care about is reducing the dimension of the feature space, and so any basis spanning a proper subspace will suffice), it is often used in exploratory data analysis in fields like statistics and the social sciences (see e.g., factor analysis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adopting the same notation as in Subsection 1.1, we have a dataset of $P$ points $\\left\\{ \\mathbf{x}_{p}\\right\\} _{p=1}^{P}$ - each of dimension $N$. Given a unit direction $\\mathbf{d}$, we can calculate the variance of data in that direction (i.e., how much the dataset spreads out in that direction) as the average squared inner product of the data against $\\mathbf{d}$\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{1}{P}\\underset{p=1}{\\overset{P}{\\sum}} \\left(\\mathbf{d}^T\\mathbf{x}_{p}\\right) ^{2}\n",
    "\\end{equation}\n",
    "\n",
    "Putting data points as columns into a matrix $\\mathbf{X}$ as in equation (6), this can be written more compactly as\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{1}{P}\\Vert\\mathbf{X}^{T}\\mathbf{d}\\Vert_{2}^{2}=\\frac{1}{P}\\mathbf{d}^{T}\\mathbf{X}\\mathbf{X}^{T}\\mathbf{d}\n",
    "\\end{equation} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the expression in (15) at hand, we can find the unit direction $\\mathbf{d}$ which maximizes it. This direction will be, by definition, the direction of maximum variance in data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us assume that the $N\\times N$ symmetric positive semi-definite matrix $\\mathbf{X}\\mathbf{X}^{T}$ has eigenvalues $\\lambda_{1}\\geq\\lambda_{2}\\geq\\cdot\\cdot\\cdot\\geq\\lambda_{N}\\geq0$ and corresponding eigenvectors $\\mathbf{a}_{1}, \\mathbf{a}_{2}, \\ldots, \\mathbf{a}_{N}$, with an eigen-decomposition\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{X}\\mathbf{X}^{T}=\\mathbf{A}\\boldsymbol{\\Lambda}\\mathbf{A}^{T}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{A}$ is an orthonormal basis whose columns are the eigenvectors, and $\\boldsymbol{\\Lambda}$ is a diagonal matrix with the corresponding eigenvalues along its diagonal. With this eigen-decomposition we can write\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{d}^{T}\\mathbf{X}\\mathbf{X}^{T}\\mathbf{d}=\\mathbf{d}^{T}\\mathbf{A}\\boldsymbol{\\Lambda}\\mathbf{A}^{T}\\mathbf{d}=\\mathbf{r}^{T}\\boldsymbol{\\Lambda}\\mathbf{r}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{r}=\\mathbf{A}^{T}\\mathbf{d}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expressing $\\mathbf{r}^{T}\\boldsymbol{\\Lambda}\\mathbf{r}$ in terms of individual entries in $\\mathbf{r}$ and $\\boldsymbol{\\Lambda}$ we have \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{d}^{T}\\mathbf{X}\\mathbf{X}^{T}\\mathbf{d}={\\sum}_{n=1}^N\\lambda_{n}r_{n}^{2}\\leq{\\sum}_{n=1}^N\\lambda_{1}r_{n}^{2}=\\lambda_{1}\\Vert\\mathbf{r}\\Vert_{2}^{2}=\\lambda_{1}\\mathbf{d}^{T}\\mathbf{A}\\mathbf{A}^{T}\\mathbf{d}=\\lambda_{1}\\mathbf{d}^{T}\\mathbf{d}=\\lambda_{1}\n",
    "\\end{equation}\n",
    "\n",
    "where we use the fact that $\\mathbf{A}$ is an orthonormal basis and $\\mathbf{d}$ has unit length.\n",
    "\n",
    "Notice, if you set $\\mathbf{d}=\\mathbf{a}_{1}$ then the objective meets its upper bound in (18), since\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{a}_{1}^{T}\\mathbf{X}\\mathbf{X}^{T}\\mathbf{a}_{1}=\\mathbf{a}_{1}^{T}\\mathbf{A}\\boldsymbol{\\Lambda}\\mathbf{A}^{T}\\mathbf{a}_{1}=\\mathbf{e}_{1}^{T}\\mathbf{\\boldsymbol{\\Lambda}e}_{1}=\\lambda_{1}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{e}_{1}=\\left[\\begin{array}{ccccc}\n",
    "1 & 0 & 0 & \\cdots & 0\\end{array}\\right]^{T}$ is the first standard basis vector, and we indeed have that\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\\,\\,\\,\\,\\,\\mathbf{a}_{1}=\\,\\, & \\underset{\\Vert\\mathbf{d}\\Vert_{2}=1}{\\mbox{argmax}}\\,\\,\\mathbf{d}^{T}\\mathbf{X}\\mathbf{X}^{T}\\mathbf{d}\n",
    "\\end{aligned}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar approach can be taken to find the second largest direction of variance of the matrix $\\mathbf{X}\\mathbf{X}^{T}$, i.e., the unit vector $\\mathbf{d}$ that maximizes the value of $\\mathbf{d}^{T}\\mathbf{X}\\mathbf{X}^{T}\\mathbf{d}$ but where $\\mathbf{d}$ is also orthogonal to $\\mathbf{a}_1$ - the first largest direction of variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now write the inequality in (18), slightly differently, as\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{d}^{T}\\mathbf{X}\\mathbf{X}^{T}\\mathbf{d}=\\lambda_{1}r_{1}^{2}+{\\sum}_{n=2}^N\\lambda_{n}r_{n}^{2}\\leq\\lambda_{2}r_{1}^{2}+{\\sum}_{n=2}^{N}\\lambda_{2}r_{n}^{2}={\\sum}_{n=1}^{N}\\lambda_{2}r_{n}^{2}\n",
    "\\end{equation}\n",
    "\n",
    "where the inequality holds since we are only looking for directions perpendicular to $\\mathbf{a}_{1}$, therefore $r_{1}=\\mathbf{d}^{T}\\mathbf{a}_{1}=0$, and we also have that $\\lambda_{2}\\geq\\cdot\\cdot\\cdot\\geq\\lambda_{N}\\geq0$ by assumption.\n",
    "\n",
    "Hence, we have \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{d}^{T}\\mathbf{X}\\mathbf{X}^{T}\\mathbf{d}\\leq{\\sum}_{n=1}^{N}\\lambda_{2}r_{n}^{2}=\\lambda_{2}\\Vert\\mathbf{r}\\Vert_{2}^{2}=\\lambda_{2}\\mathbf{d}^{T}\\mathbf{A}\\mathbf{A}^{T}\\mathbf{d}=\\lambda_{2}\\mathbf{d}^{T}\\mathbf{d}=\\lambda_{2}\n",
    "\\end{equation}\n",
    "\n",
    "Notice, this time, the upper bound is met with $\\mathbf{d}=\\mathbf{a}_{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can show that the rest of the orthogonal directions of maximum variance in data are the remaining eigenvectors of $\\mathbf{X}\\mathbf{X}^{T}$, which are precisely the singular value solution given previously in equation (13). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see why this is true, we expand $\\mathbf{XX}^{T}$ using its singular value decomposition $\\mathbf{X}=\\mathbf{U}\\boldsymbol{S}\\mathbf{V}^{T}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\mathbf{XX}^{T}=\\mathbf{U}\\boldsymbol{S}\\mathbf{V}^{T}\\left(\\mathbf{U}\\boldsymbol{S}\\mathbf{V}^{T}\\right)^{T}=\\mathbf{U}\\boldsymbol{S}\\mathbf{V}^{T}\\left(\\mathbf{V}\\boldsymbol{S}^{T}\\mathbf{U}^{T}\\right)=\\mathbf{U}\\boldsymbol{S}\\left(\\mathbf{V}^{T}\\mathbf{V}\\right)\\boldsymbol{S}^{T}\\mathbf{U}^{T}=\\mathbf{U}\\boldsymbol{S}\\boldsymbol{S}^{T}\\mathbf{U}^{T}\n",
    "\\end{equation}\n",
    "\n",
    "Given the eigen-decomposition of $\\mathbf{X}\\mathbf{X}^{T}$ as $\\mathbf{X}\\mathbf{X}^{T}=\\mathbf{A}\\boldsymbol{\\Lambda}\\mathbf{A}^{T}$ in equation (16), a simple inspection reveals that $\\mathbf{U}=\\mathbf{A}$ and $\\mathbf{S}\\mathbf{S}^{T}=\\boldsymbol{\\Lambda}$."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "214px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 1,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
