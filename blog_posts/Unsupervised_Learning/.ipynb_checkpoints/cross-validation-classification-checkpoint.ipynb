{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Nonlinear Supervised Learning Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n the previous chapter we saw how using more basis elements generally results in a better approximation of a continuous function. However, as we saw with regression in Section 5.3, while it is true that our approximation of a dataset itself improves as we add more basis features, this can substantially decrease our estimation of the underly- ing data generating function (a phenomenon referred to as overfitting). Unfortunately, the same overfitting problem presents itself in the case of classification as well. Sim- ilarly to regression, in the ideal classification scenario discussed in Section 6.1 using more basis elements generally improves our approximation. However, in general in- stances of classification, analogous to what we saw with regression, adding more basis features (increasing M) can result in fitting closely to the data we have while poorly to the underlying function (a phenomenon once again referred to as overfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We illustrate the overfitting issue with classification using a particular dataset in Fig. 6.11, where we show the discretized indicator (left panel) along with the related noisy dataset (right panel) originally shown together in Fig. 6.5. For each dataset we show the resulting fit provided by both a degree 2 and a degree 5 polynomial (shown in black and green respectively). While the degree 2 features produce a classifier in each case that closely matches the true boundary, the higher degree 5 polynomial creates an overfitting classifier which encapsulates mislabeled points outside of the half circle boundary of the true function, leading to a poorer representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src= '../../mlrefined_images/unsupervised_images/Fig_6_11.png' width=\"60%\" height=\"auto\" alt=\"\"/>\n",
    "  <figcaption>   \n",
    "<strong>Figure 6.11:</strong> <em> Discretized (left panel) and noisy samples (right) from a generating function with class boundary shown (the circle of radius one-half) in dashed black, along with fits of degree 2 (in black) and degree 5 (in green) polynomial features. Like regression, while increasing the number of basis elements produces a better fit in the discretized case, for more realistic cases like the dataset on the right this can lead to overfitting. In the right panel the lower degree polynomial produces a classifier that matches the true boundary fairly well, while the higher degree polynomial leads to a classifier that overfits the data encapsulating falsely labeled red points outside the true boundary, thus leading to a poorer representation of the underlying generating function (see text for further details).</em>  </figcaption> \n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we outline the use of cross-validation, culminating once again with the k-fold cross-validation method, for the intelligent automatic choice of M for both two class and multiclass classification problems. As with regression, here once again the k-fold method8 provides a way of determining a proper value of M, however, once again this comes at significant computational cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**footnote:** Readers particularly interested in using fixed bases with high dimensional input, deep network features, as well as multiclass softmax classification using feature bases should also see Section 7.3, where a variation of k-fold cross-validation is introduced that is more appropriate for these instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hold out cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analogous to regression, in the case of classification ideally we would like to choose a number M of basis features so that the corresponding learned representation matches the true data generating indicator function as well as possible. Of course because we only have access to this true function via (potentially) noisy samples, this goal must be pursued based solely on the data. Therefore once again we aim to choose M such that the corresponding model fits both the data we currently have, as well as the data we might receive in the future. Because we do not have access to any future data points this intuitively directs us to employ cross-validation, where we tune M, so that the corresponding model fits well to an unseen portion of our original data (i.e., the testing set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we can do precisely what was described for regression in Section 5.3 and per- form k-fold cross-validation to determine M. In other words, we can simulate this desire by splitting our original data into k evenly sized pieces and merge k − 1 of them into a training set and use the remaining piece as a testing set. Furthermore the same intu- ition for choosing k introduced for regression also holds here, with common values in practice being in the range k = 3–10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#a50e3e;\">Example 1: </span> Hold out for classification using polynomial features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For clarity we first show an example of the hold out method, followed by explicit computations, which are then simply repeated on each fold (averaging the results) in performing the k-fold method. In Fig. 6.12 we show the result of applying hold out cross-validation to the dataset first shown in the bottom panels of Fig. 6.5. Here we use k = 3, use the softmax cost, and M in the range M = 2,5,9,14,20,27,35,44 which corresponds (see footnote 5) to polynomial degrees D = 1, 2, . . . , 8 (note that for clarity panels in the figure are indexed by D)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src= '../../mlrefined_images/unsupervised_images/Fig_6_12.png' width=\"60%\" height=\"auto\" alt=\"\"/>\n",
    "  <figcaption>   \n",
    "<strong>Figure 6.12:</strong> <em> An example of hold out cross-validation applied using polynomial features. (left panel) The original data split into training and testing sets, with the points belonging to each set drawn as smaller thick and larger thin points respectively. (middle eight panels) The fit resulting from each set of degree D polynomial features in the range D = 1, 2, . . . , 8 shown in black in each panel. Note how the lower degree fits underfit the data, while the higher degree fits overfit the data. (second from right panel) The training and testing errors, in blue and yellow respectively, of each fit over the range of degrees tested. From this we see that D⋆ = 4 (or M⋆ = 14) provides the best fit. Also note how the training error always decreases as we increase the degree/number of basis elements, which will always occur regardless of the dataset/feature basis type used. (right panel) The final model using M⋆ = 14, trained on the entire dataset (shown in black), fits the data well and closely matches the boundary of the underlying data generating function (shown in dashed black).</em>  </figcaption> \n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the models learned for each value of M (see the middle set of eight pan- els of the figure) we plot training and testing errors (in the panel second to the right), measuring how well each model fits the training and testing data respectively, over the entire range of values. Note that unlike the testing error, the training error always de- creases as we increase M (which occurs more generally regardless of the dataset/feature basis used). The model that provides the smallest testing error (M⋆ = 14 or equivalently D⋆ = 4) is then trained again on the entire dataset, giving the final classification model shown in black in the rightmost panel of the figure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hold-out calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we give a complete set of holdout cross-validation calculations in a general setting, which closely mirrors the version given for regression in Section [subsec:Hold-out-calculations-regression]. We denote the collection of points belonging to the training and testing sets respectively by their indices as\\noindent\\begin{array}{c}\n",
    "\\Omega_{\\textrm{train}}=\\left\\{ p\\,\\vert\\,\\left(\\mathbf{x}_{p},\\,y_{p}\\right)\\,\\mbox{belongs to the training set}\\right\\} \\\\\n",
    "\\Omega_{\\textrm{test}}=\\left\\{ p\\,\\vert\\,\\left(\\mathbf{x}_{p},\\,y_{p}\\right)\\,\\mbox{belongs to the testing set}\\right\\} \n",
    "\\end{array}We then choose a basis type (e.g., polynomial, Fourier, neural network) and choose a range for the number of basis features over which we search for an ideal value for M. To determine the training and testing error of each value of M tested we first form the corresponding feature vector \\mathbf{f}_{p}=\\left[\\begin{array}{cccc}\n",
    "f_{1}\\left(\\mathbf{x}_{p}\\right) & f_{2}\\left(\\mathbf{x}_{p}\\right) & \\cdots & f_{M}\\left(\\mathbf{x}_{p}\\right)\\end{array}\\right]^{T} and fit a corresponding model to the training set by minimizing e.g., the softmax or squared margin cost. For example employing the softmax we solve\\noindent\\underset{b,\\,\\mathbf{w},\\Theta}{\\mbox{minimize}}\\,\\,\\underset{p\\in\\Omega_{\\textrm{train}}}{\\sum}\\mbox{log}\\left(1+e^{-y_{p}\\left(b+\\mathbf{f}_{p}^{T}\\mathbf{w}\\right)}\\right).Denoting a solution to the problem above as \\left(b_{M}^{\\star},\\,\\mathbf{w}_{M}^{\\star},\\,\\Theta_{M}^{\\star}\\right) we find the training and testing errors for the current value of M using these parameters over the training and testing sets using the counting cost (see Section [subsec:nonlinear-classifier-accuracy-and-boundary]), respectively\\noindent\\begin{array}{c}\n",
    "\\begin{array}{c}\n",
    "\\mbox{Training error}=\\frac{1}{\\left|\\Omega_{\\textrm{train}}\\right|}\\underset{p\\in\\Omega_{\\textrm{train}}}{\\sum}\\mbox{max}\\left(0,\\,\\,\\mbox{sign}\\left(-y_{p}\\left(b^{\\star}+\\mathbf{f}_{p}^{T}\\mathbf{w}^{\\star}\\right)\\right)\\right)\\\\\n",
    "\\mbox{Testing error}=\\frac{1}{\\left|\\Omega_{\\textrm{test}}\\right|}\\underset{p\\in\\Omega_{\\textrm{test}}}{\\sum}\\mbox{max}\\left(0,\\,\\,\\mbox{sign}\\left(-y_{p}\\left(b^{\\star}+\\mathbf{f}_{p}^{T}\\mathbf{w}^{\\star}\\right)\\right)\\right),\n",
    "\\end{array}\\end{array}where the notation \\left|\\Omega_{\\textrm{train}}\\right| and \\left|\\Omega_{\\textrm{test}}\\right| denotes the cardinality or number of points in the training and testing sets, respectively. Once we have performed these calculations for all values of M we wish to test, we choose the one that provides the lowest testing error, denoted by M^{\\star}. \n",
    "\n",
    "Finally we form the feature vector \\mathbf{f}_{p}=\\left[\\begin{array}{cccc}\n",
    "f_{1}\\left(\\mathbf{x}_{p}\\right) & f_{2}\\left(\\mathbf{x}_{p}\\right) & \\cdots & f_{M^{\\star}}\\left(\\mathbf{x}_{p}\\right)\\end{array}\\right]^{T} for all the points in the entire dataset, and solve the following optimization problem over the entire dataset to form the final model\\noindent\\underset{b,\\,\\mathbf{w},\\Theta}{\\mbox{minimize}}\\,\\,\\underset{p=1}{\\overset{P}{\\sum}}\\mbox{log}\\left(1+e^{-y_{p}\\left(b+\\mathbf{f}_{p}^{T}\\mathbf{w}\\right)}\\right)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-fold cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As introduced in Section 5.3.4, k-fold cross-validation is a robust extension of the hold out method whereby the procedure is repeated k times where in each instance (or fold) we treat a different portion of the split as a testing set and the remaining k − 1 portions as the training set. The hold out calculations are then made, as detailed previously, on each fold and the value of M with the lowest average testing error is chosen. This produces a more robust choice of M, because potentially poor hold out choices on individual folds can be averaged out, producing a stronger model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#a50e3e;\">Example 2: </span> k-fold cross-validation for classification using polynomial features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Fig. 6.13 we illustrate the result of applying k-fold cross-validation to choose the ideal number M of polynomial features for the dataset shown in Example 6.5, where it was originally used to illustrate the hold out method. As in the previous example, here we set k = 3, use the softmax cost, and try M in the range M = 2,5,9,14,20,27,35,44 which corresponds (see footnote 5) to polynomial degrees D = 1, 2, . . . , 8 (note that for clarity panels in the figure are indexed by D)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src= '../../mlrefined_images/unsupervised_images/Fig_6_13.png' width=\"60%\" height=\"auto\" alt=\"\"/>\n",
    "  <figcaption>   \n",
    "<strong>Figure 6.13:</strong> <em> Result of performing k-fold cross-validation with k = 3 (see text for further details). The top three rows display the result of performing the hold out method on each fold. The left, middle, and right columns show each fold’s training/testing sets (drawn as thick and thin points respectively), training and testing errors over the range of M tried, and the final model (fit to the entire dataset) chosen by picking the value of M providing the lowest testing error. Due to the split of the data, performing hold out on each fold results in a poor overfitting (first two folds) or underfitting (final fold) model for the data. However, as illustrated in the final row, by averaging the testing errors (bottom middle panel) and choosing the model with minimum associated average test error we average out these problems (finding that D⋆ = 4 or M⋆ = 14) and determine an excellent model for the phenomenon (as shown in the bottom right panel). </em>  </figcaption> \n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the top three rows of Fig. 6.13 we show the result of applying hold out on each fold. In each row we show a fold’s training and testing data in the left panel, the training/testing errors for each M on the fold (as computed in Equation (6.25)) in the middle panel, and the final model (learned to the entire dataset) provided by the choice of M with lowest testing error. As can be seen, the particular split leads to an overfitting result on the first two folds and an underfitting result on the third fold. In the middle panel of the final row we show the result of averaging the train- ing/testing errors over all k = 3 folds, and in the right panel the result of choosing the overall best M⋆ = 14 (or equivalently D⋆ = 4) providing the lowest average testing error. By taking this value we average out the poor choices determined on each fold, and end up with a model that fits both the data and underlying function quite well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#a50e3e;\">Example 3: </span> Warning examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a k-fold determined set of features performs poorly this is almost always indicative of a poorly structured dataset (i.e., there is little relationship between the input/output data), like the one shown in the left panel of Fig. 6.14. However, there are also instances, when we have too little or too poorly distributed data, when a high per- forming k-fold model can be misleading as to how well we understand a phenomenon. In the middle and right panels of the figure we show two such instances that the reader should keep in mind when using k-folds, where we either have too little (middle panel) or poorly distributed data (right panel)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src= '../../mlrefined_images/unsupervised_images/Fig_6_14.png' width=\"60%\" height=\"auto\" alt=\"\"/>\n",
    "  <figcaption>   \n",
    "<strong>Figure 6.14:</strong> <em> (left panel) A low accuracy k-fold fit to a dataset indicates that it has little structure (i.e., that there is little to no relationship between the input and output). It is possible that a high accuracy k-fold fit fails to capture the true nature of an underlying function, as when (middle panel) we have too little data (the k-fold linear separator is shown in black, and the true nonlinear separator is shown dashed) and (right panel) when we have poorly distributed data (again the k-fold separator is shown in black, the original separator dashed). See text for further details. </em>  </figcaption> \n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first instance we have generated a small sample of points based on the second indicator function shown in Fig. 6.3, which has a nonlinear boundary in the original feature space. However, the sample of data is so small that it is perfectly linearly sep- arable, and thus applying e.g., k-fold cross-validation with polynomial basis features will properly (due to the small selection of data) recover a line to distinguish between the two classes. However, clearly data generated via the same underlying process in the future will violate this linear boundary, and thus our model will perform poorly. This sort of problem arises in applications such as automatic medical diagnosis (see Example 1.6) where access to data is limited. Unless we can gather additional data to fill out the space (making the nonlinear boundary more visible) this problem is unavoidable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second instance shown in the right panel of the figure, we have plenty of data (generated using the indicator function originally shown in Fig. 6.4) but it is poorly distributed. In particular, we have no samples from the blue class in the lower half of the space. In this case the k-fold method (again here using polynomial features) properly determines a separating boundary that perfectly distinguishes the two classes. However, many of the blue class points we would receive in the future in the lower half of the space will be misclassified given the learned k-fold model. This sort of issue can arise in practice, e.g., when performing face detection (see Example 1.4), if we do not collect a thorough dataset of blue (e.g., “non-face”) examples. Again, unless we can gather further data to fill out the space this problem is unavoidable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-fold cross-validation for one-versus-all multiclass classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Employing the one-versus-all (OvA) framework for multiclass classification, we can immediately apply the k-fold method described previously. For a C class problem we simply apply the k-fold method to each of the C two class classification problems, and combine the resulting classifiers as shown in Equation (6.21). We show the result of applying k = 3 fold cross-validation with OvA on two datasets with C = 3 and C = 5 classes respectively in Fig. 6.15 and 6.16, where we have used polynomial features with M = 2,5,9,14,20,27,35,44 or equivalently of degree D = 1,2,...,8 for each two class subproblem. Displayed in each figure are the nonlinear boundaries determined for each fold, as well as the combined result in the right panel of each figure. In both instances the combined boundaries separate the different classes of data very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src= '../../mlrefined_images/unsupervised_images/Fig_6_15.png' width=\"60%\" height=\"auto\" alt=\"\"/>\n",
    "  <figcaption>   \n",
    "<strong>Figure 6.15:</strong> <em> Result of performing k = 3 fold cross-validation on the C = 3 class dataset first shown in\n",
    "Fig. 6.10 using OvA (see text for further details). The left three panels show the result for the red class versus all, blue class versus all, and green class versus all subproblems. For the red/green versus all problems the optimal degree found was D⋆ = 2, while for the blue versus all D⋆ = 4 (note how this produces a better fit than the D = 2 fit shown originally in Fig. 6.10). The right panel shows the combined boundary determined by Equation (6.21), which perfectly separates the three classes. </em>  </figcaption> \n",
    "</figure>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "214px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 1,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
